{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import feedparser as fp\n",
    "import newspaper as np\n",
    "\n",
    "def mettre_a_jour_base_de_donnees(flux_rss_a_lire, database, categories):\n",
    "    # Liste des nouveaux articles\n",
    "    new_articles = []\n",
    "\n",
    "    # Parcourir les flux RSS\n",
    "    for source_url in flux_rss_a_lire:\n",
    "        # Déterminer la catégorie du flux\n",
    "        category = categories[source_url]\n",
    "\n",
    "        data = fp.parse(source_url)\n",
    "        \n",
    "        # Parcourir les articles du flux RSS\n",
    "        for item in data.entries:\n",
    "            # Clés renommées pour correspondre à la base de données existante\n",
    "            article = {\n",
    "                \"title\": item.title,\n",
    "                \"date\": item.published,  # Renommé de 'published' à 'date'\n",
    "                \"url\": item.link,  # Renommé de 'link' à 'url'\n",
    "                \"content\": item.description if \"description\" in item else \"\",\n",
    "                \"image_link\": item.enclosures[0].url if \"enclosures\" in item and item.enclosures[0].type == \"image/jpeg\" else \"\",\n",
    "                \"category\": category,  # Ajouté la catégorie\n",
    "            }\n",
    "            \n",
    "            # Vérifier si l'article existe déjà dans la base de données\n",
    "            if article[\"url\"] not in database:\n",
    "                article_obj = np.Article(article[\"url\"])\n",
    "                article_obj.download()\n",
    "                article_obj.parse()\n",
    "                \n",
    "                # Mettre à jour les informations de l'article\n",
    "                article[\"content\"] += \"\\n\\n\" + article_obj.text  # Ajouter le contenu téléchargé\n",
    "                article[\"author\"] = article_obj.authors  # Ajouté la liste des auteurs\n",
    "                \n",
    "                # Ajouter l'article à la liste des nouveaux articles\n",
    "                new_articles.append((article[\"url\"], article))\n",
    "\n",
    "    # Mettre à jour la base de données existante avec les nouveaux articles\n",
    "    for url, article in new_articles:\n",
    "        database[url] = article\n",
    "\n",
    "    return database\n",
    "\n",
    "\n",
    "# Lien vers le fichier tvinfo-sources.json\n",
    "tvinfo_sources_url = \"https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/tvinfo-sources.json\"\n",
    "\n",
    "# Charger la liste des flux RSS depuis le lien tvinfo-sources.json\n",
    "response_sources = requests.get(tvinfo_sources_url)\n",
    "sources = json.loads(response_sources.text)\n",
    "\n",
    "# Liste des flux RSS à parcourir\n",
    "flux_rss_a_lire = list(sources.values())\n",
    "\n",
    "# Usage de la fonction avec les catégories\n",
    "categories = {\n",
    "    \"https://www.francetvinfo.fr/france.rss\": \"france\",\n",
    "    \"https://www.francetvinfo.fr/monde/europe.rss\": \"europe\",  # Corrigé ici\n",
    "    \"https://www.francetvinfo.fr/economie/entreprises.rss\": \"economie\"\n",
    "}\n",
    "\n",
    "\n",
    "# Charger la base de données existante depuis le lien francetvinfo.json\n",
    "francetvinfo_url = \"https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/francetvinfo.json\"\n",
    "response_database = requests.get(francetvinfo_url)\n",
    "database = json.loads(response_database.text)\n",
    "\n",
    "# Mettre à jour la base de données avec les nouveaux articles\n",
    "database_mise_a_jour = mettre_a_jour_base_de_donnees(flux_rss_a_lire, database, categories)\n",
    "\n",
    "# Transformer la base de données mise à jour en DataFrame\n",
    "df_database_mise_a_jour = pd.DataFrame(database_mise_a_jour.values())\n",
    "\n",
    "# Ajouter les URLs comme colonne dans le DataFrame\n",
    "df_database_mise_a_jour['url'] = list(database_mise_a_jour.keys())\n",
    "\n",
    "# Afficher les premières lignes pour vérifier\n",
    "df_database_mise_a_jour\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10fc60",
   "metadata": {},
   "source": [
    "# Mettre à jour la base de données de Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction pour récupérer la base de données actuelle depuis GitHub\n",
    "def telecharger_base_donnees_github(url):\n",
    "    response = requests.get(url)\n",
    "    data = response.text.splitlines()  # Séparer chaque ligne du fichier JSON\n",
    "    df = pd.DataFrame([pd.read_json(line, typ='series') for line in data])\n",
    "    return df\n",
    "\n",
    "# Fonction pour concaténer l'ancienne base de données avec les nouveaux articles\n",
    "def concatener_bases_donnees(df_ancienne, df_nouveaux_articles):\n",
    "    # Filtrer les nouveaux articles pour ne garder que ceux qui n'existent pas déjà dans l'ancienne base de données\n",
    "    nouveaux_articles_uniques = df_nouveaux_articles[~df_nouveaux_articles['url'].isin(df_ancienne['url'])]\n",
    "    # Concaténer les deux DataFrames\n",
    "    df_mise_a_jour = pd.concat([df_ancienne, nouveaux_articles_uniques], ignore_index=True)\n",
    "    return df_mise_a_jour\n",
    "\n",
    "# URL de la base de données sur GitHub\n",
    "url_github = 'https://raw.githubusercontent.com/diakitegaoussou4996/ENSAI-PROJET-WEBMING/main/Data/francetvinfo_maj.json'\n",
    "\n",
    "# Charger la base de données actuelle\n",
    "df_ancienne = telecharger_base_donnees_github(url_github)\n",
    "\n",
    "# Concaténer l'ancienne base de données avec les nouveaux articles\n",
    "df_mise_a_jour = concatener_bases_donnees(df_ancienne, df_database_mise_a_jour)\n",
    "\n",
    "# Sauvegarder la base de données mise à jour localement\n",
    "df_mise_a_jour.to_json(\"Data/francetvinfo_maj.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_mise_a_jour.to_csv(\"Data/francetvinfo_maj.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mise_a_jour"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6102b5d",
   "metadata": {},
   "source": [
    "git add Data/francetvinfo_maj.json Data/francetvinfo_maj.csv\n",
    "git commit -m \"Mise à jour de la base de données des articles\"\n",
    "git push\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
